{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_pure.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqZuUjD1eDW9jC6bJ6P9qR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartyPants042/NER-Deep-Learning/blob/master/NER_pure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-MJfWamr89R",
        "colab_type": "text"
      },
      "source": [
        "# Data Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI5gDAeXjhQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/SmartyPants042/NER-Deep-Learning.git\n",
        "pwd\n",
        "cd NER-Deep-Learning/Data/\n",
        "unzip dataset.zip\n",
        "unzip test_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJoFMApKteqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_input = '/content/NER-Deep-Learning/Data/dataset.csv'\n",
        "test_input = '/content/NER-Deep-Learning/Data/test_dataset.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ2wyW6SoQA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataframe manipulations\n",
        "import pandas as pd\n",
        "# Array manipulations\n",
        "import numpy as np\n",
        "\n",
        "# Library used for deep learning\n",
        "import tensorflow as tf\n",
        "# Not all sentences are of same length, padding is required\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Converts the target labels to categories that the neural net can predict\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Type of model used for DL\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Layers present in the network. \n",
        "# Refer README.md for more details.\n",
        "# Analysis of different combinations of layers in README.md\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDe4Qyj9otui",
        "colab_type": "text"
      },
      "source": [
        "# `Data Manipulation`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwzL741toq1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(data_input)\n",
        "df_test = pd.read_csv(test_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuW9iE71o1Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregate_function = lambda s: [(w, p, t) for w, p, t in zip(\n",
        "    list(s['Word'].values),\n",
        "    list(s['POS Tag'].values),\n",
        "    list(s['NER Tag'].values)\n",
        ")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNgmmYYo1rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.groupby(\"Sentence ID\").apply(aggregate_function)\n",
        "sentences_test = df_test.groupby(\"Sentence ID\").apply(aggregate_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7qXijCoo6N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max(len(s) for s in sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrSxGr08pEK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = list(set(df[\"Word\"].values))\n",
        "words.append(\"__PAD__\")\n",
        "n_words = len(words)\n",
        "print(n_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0y0a2WepHJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = list(set(df[\"NER Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "print(n_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oPU8etqpJsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2id = {w: i for i, w in enumerate(words)}\n",
        "tag2id = {t: i for i, t in enumerate(tags)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAEUuKi5pNfO",
        "colab_type": "text"
      },
      "source": [
        "# `Training & Testing`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX0wrNvspJ2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates vector of sentences, where each sentence is itself a vector of 62 words maximum.\n",
        "# We have not yet converted the words to thier respective IDs.\n",
        "# We have not yet made the sentences of the same length also known as padding.\n",
        "X_train_sent = [[tup[0] for i, tup in enumerate(sent) if i<max_length] for sent in sentences]\n",
        "y_train_sent = [[tup[2] for i, tup in enumerate(sent) if i<max_length] for sent in sentences]\n",
        "X_test_sent = [[tup[0] for i, tup in enumerate(sent) if i<max_length] for sent in sentences_test]\n",
        "y_test_sent = [[tup[2] for i, tup in enumerate(sent) if i<max_length] for sent in sentences_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERsjctJgpXBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_encodings(X_sent, y_sent):\n",
        "    \"\"\"\n",
        "    Description: Converts the list of sentences containing words to a list of sentences conataining just numbers.\n",
        "    If the word is present in the vocabulary, it is assigned the correct corresponding id;\n",
        "    If the word is present in the twitter data, but not in the GMB data,\n",
        "    we simply assign it the value of '__PAD__'.\n",
        "    \n",
        "    Returns: Two tuple of encoded sentences, encoded target labels\n",
        "    \n",
        "    Input Params: Two tuple of sentences and target labels.\n",
        "    \"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    for x_s, y_s in zip(X_sent, y_sent):\n",
        "\n",
        "        temp_x = []\n",
        "        temp_y = []\n",
        "\n",
        "        for x, y in zip(x_s, y_s):\n",
        "            try:\n",
        "                x = word2id[x]\n",
        "            except:\n",
        "                x = word2id['__PAD__']\n",
        "            try:\n",
        "                y = tag2id[y]\n",
        "            except:\n",
        "                y = tag2id['O']\n",
        "\n",
        "            temp_x.append(x)\n",
        "            temp_y.append(y)\n",
        "        \n",
        "        temp_x = np.array(temp_x)\n",
        "        temp_y = np.array(temp_y)\n",
        "        \n",
        "        X_train.append(temp_x)\n",
        "        y_train.append(temp_y)\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    return (X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOt2WnjpgYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = generate_encodings(X_train_sent, y_train_sent)\n",
        "X_test, y_test = generate_encodings(X_test_sent, y_test_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoPDzx-Jph7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make the sentences and the target labels of each of the same length, 62.\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post', value=word2id['__PAD__'])\n",
        "y_train = pad_sequences(y_train, maxlen=max_length, padding='post', value=tag2id['O'])\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post', value=word2id['__PAD__'])\n",
        "y_test = pad_sequences(y_test, maxlen=max_length, padding='post', value=tag2id['O'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAjQiGcCpoq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = [to_categorical(i, n_tags) for i in y_train]\n",
        "y_test = [to_categorical(i, n_tags) for i in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBJjJIPpuQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "                    Embedding(input_dim=n_words, output_dim=64),\n",
        "                    Dropout(0.1),\n",
        "                    Bidirectional(LSTM(\n",
        "                        128,\n",
        "                        activation='tanh', \n",
        "                        return_sequences=True, \n",
        "                        recurrent_activation='sigmoid', \n",
        "                        use_bias=True,\n",
        "                        )),\n",
        "                    TimeDistributed(Dense(\n",
        "                        n_tags,\n",
        "                        activation='softmax'\n",
        "                    ))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jasXEeM-pxkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zq79kA6p0mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, np.array(y_train), batch_size=64, epochs=3, verbose=1, validation_data=(X_test, np.array(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}